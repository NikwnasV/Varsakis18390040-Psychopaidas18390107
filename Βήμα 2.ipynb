{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d191f63-bf24-44f8-81cc-29e65dddd951",
   "metadata": {},
   "source": [
    "Όπως και στο πρώτο βήμα, θα κάνουμε πρωτά import όσα libraries χρησιμοποίησουμε. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e53a11-44c0-490f-b986-9fe0c7c0f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ae918-312b-49da-81e5-e4399249863f",
   "metadata": {},
   "source": [
    "Στη συνέχεια, θα βεβαιώσουμε ότι τα απαραίτητα αρχεία από τη βιβλιοθήκη nltk είναι διάθεσιμα, και σε δεύτερο χρόνο, θα αρχικοποιήσουμε τα απαραίτητα εργαλεία για την προεπεξεργασία των .csv αρχείων που δημιουργήσαμε νωρίτερα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6265cb35-08ee-4213-97b2-17aa0c17eaf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Agisilaos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Agisilaos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Agisilaos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Τα αρχεία nltk \n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Η αρχικοποίηση των εργαλείων προεπεξεργασίας.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa0fa4-853f-488f-9e7e-aa27ef9ea3a8",
   "metadata": {},
   "source": [
    "Καθώς έχουμε τα αρχεία, δεν χρειάζεται κάποια περαιτέρω ενέργεια, και είμαστε έτοιμοι να προχωρήσουμε. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37313e82-3817-496c-8b7a-8521338d3b22",
   "metadata": {},
   "source": [
    "Θα δημιουργήσουμε μια συνάρτηση η οποία όταν καλείται με παράμετρο το κείμενο που ύπαρχει μέσα στο αρχείο .csv, κάνει την επεξεργασία για να αφαιρέσει τους ειδικούς χαρακτήρες και τα stopwords, και να απλοποίησει τη μορφή του κειμένου, και δίνει ως έξοδο το επεξεργασμένο κείμενο ως ένα string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387ca5ba-4389-47a0-836a-3e6da58f57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function we created\n",
    "def process_text(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Return the cleaned text as a single string\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d640bb-0785-450e-b1c3-e8498b2b1b01",
   "metadata": {},
   "source": [
    "Εχόντας αυτή τη συνάρτηση, μπορούμε να είσαγουμε τα αποτελέσματα αυτής της επεξεργασίας, σε ένα νεό .csv αρχείο με τον ακόλουθο κώδικα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "471d5921-357b-4d14-9bc0-98ab759c032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia_article_1.csv...\n",
      "Processed data saved to processed_wikipedia_article_1.csv\n",
      "Processing wikipedia_article_10.csv...\n",
      "Processed data saved to processed_wikipedia_article_10.csv\n",
      "Processing wikipedia_article_11.csv...\n",
      "Processed data saved to processed_wikipedia_article_11.csv\n",
      "Processing wikipedia_article_12.csv...\n",
      "Processed data saved to processed_wikipedia_article_12.csv\n",
      "Processing wikipedia_article_13.csv...\n",
      "Processed data saved to processed_wikipedia_article_13.csv\n",
      "Processing wikipedia_article_14.csv...\n",
      "Processed data saved to processed_wikipedia_article_14.csv\n",
      "Processing wikipedia_article_15.csv...\n",
      "Processed data saved to processed_wikipedia_article_15.csv\n",
      "Processing wikipedia_article_16.csv...\n",
      "Processed data saved to processed_wikipedia_article_16.csv\n",
      "Processing wikipedia_article_17.csv...\n",
      "Processed data saved to processed_wikipedia_article_17.csv\n",
      "Processing wikipedia_article_18.csv...\n",
      "Processed data saved to processed_wikipedia_article_18.csv\n",
      "Processing wikipedia_article_19.csv...\n",
      "Processed data saved to processed_wikipedia_article_19.csv\n",
      "Processing wikipedia_article_2.csv...\n",
      "Processed data saved to processed_wikipedia_article_2.csv\n",
      "Processing wikipedia_article_20.csv...\n",
      "Processed data saved to processed_wikipedia_article_20.csv\n",
      "Processing wikipedia_article_21.csv...\n",
      "Processed data saved to processed_wikipedia_article_21.csv\n",
      "Processing wikipedia_article_3.csv...\n",
      "Processed data saved to processed_wikipedia_article_3.csv\n",
      "Processing wikipedia_article_4.csv...\n",
      "Processed data saved to processed_wikipedia_article_4.csv\n",
      "Processing wikipedia_article_5.csv...\n",
      "Processed data saved to processed_wikipedia_article_5.csv\n",
      "Processing wikipedia_article_6.csv...\n",
      "Processed data saved to processed_wikipedia_article_6.csv\n",
      "Processing wikipedia_article_7.csv...\n",
      "Processed data saved to processed_wikipedia_article_7.csv\n",
      "Processing wikipedia_article_8.csv...\n",
      "Processed data saved to processed_wikipedia_article_8.csv\n",
      "Processing wikipedia_article_9.csv...\n",
      "Processed data saved to processed_wikipedia_article_9.csv\n"
     ]
    }
   ],
   "source": [
    "# Process CSV files in the current directory\n",
    "for file_name in os.listdir('.'):  # Change to the directory where your CSV files are stored, if needed.\n",
    "    if file_name.endswith('.csv'):\n",
    "        print(f\"Processing {file_name}...\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_name)\n",
    "\n",
    "        # Ensure the CSV has a 'Paragraph' column\n",
    "        if 'Paragraph' in df.columns:\n",
    "            # Process each paragraph\n",
    "            processed_data = []\n",
    "            for paragraph in df['Paragraph']:\n",
    "                processed = process_text(paragraph)\n",
    "                processed_data.append({'Paragraph': processed})\n",
    "\n",
    "            # Create a new DataFrame with processed data\n",
    "            processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "            # Save the processed data to a new CSV file\n",
    "            processed_file_name = f\"processed_{file_name}\"\n",
    "            processed_df.to_csv(processed_file_name, index=False, encoding='utf-8')\n",
    "            print(f\"Processed data saved to {processed_file_name}\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_name}: 'Paragraph' column not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e529736-d4aa-4c0b-9846-c4ad2224717b",
   "metadata": {},
   "source": [
    "Οπως φαίνεται και από τα αποτελέσματα, τα αρχεία μας πέρασαν με επιτυχία τη διαδικασία, και είμαστε πλέον ετοιμοί να προχωρήσουμε στο βήμα 3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
