{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87380f55-a2b3-4f71-9d85-d0f1562c7b1d",
   "metadata": {},
   "source": [
    "Στο πρώτο κομμάτι του κώδικα, θα κάνουμε import τα απαραίτητα libraries για το webcrawler πρόγραμμα μας. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ddc6649-b87e-4243-a70c-0a720c3a5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7325b8-2eff-4537-89af-1acf9315d5f9",
   "metadata": {},
   "source": [
    "Στη συνέχεια, με μια επαναληπτική δόμη, το webcrawler δέχεται τον αριθμό των άρθρων τα οποία θα κανει parse, και στη συνέχεια, το URL απο τη σελίδα της wikipedia από το κάθε άρθρο. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56cf717-aeb2-4026-9094-1d31c4a7792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the number of articles you want to parse:  3\n",
      "Enter the last part of the Wikipedia URL for article 1 (e.g.,For the 'https://en.wikipedia.org/wiki/BMW') URL, enter BMW \n",
      " BMW\n",
      "Enter the last part of the Wikipedia URL for article 2 (e.g.,For the 'https://en.wikipedia.org/wiki/BMW') URL, enter BMW \n",
      " Volkswagen\n",
      "Enter the last part of the Wikipedia URL for article 3 (e.g.,For the 'https://en.wikipedia.org/wiki/BMW') URL, enter BMW \n",
      " Audi\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to enter the number of articles to parse\n",
    "n = int(input(\"Please enter the number of articles you want to parse: \"))\n",
    "for i in range(n):\n",
    "    # Ο χρήστης καλε΄ίται να συμπληρώσει μόνο το τελευταίο μέρος του URL, δηλαδή το όνομα του άρθρου που ψάχνει.\n",
    "    article_name = input(f\"Enter the last part of the Wikipedia URL for article {i + 1} (e.g.,For the 'https://en.wikipedia.org/wiki/BMW') URL, enter BMW \\n\")\n",
    "    url = f\"https://en.wikipedia.org/wiki/{article_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d79ff-7527-4233-b361-df1dabc4adea",
   "metadata": {},
   "source": [
    "Για παράδειγμα της λειτουργίας του προγράμματος στο notebook, έχει γίνει εκτέλεση του κώδικα με τρεις εισαγωγές. Για τα επόμενα βήματα της εργασίας, το σύνολο δεδομένων αποτελείται απο είκοσι ένα διαφορετικά άρθρα. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d7e0b-bd33-4c84-a193-0ca424f65f71",
   "metadata": {},
   "source": [
    "Η επιλογή να γίνει χρήση επαναληπτικής δομής δεν είναι απαραίτητη, αλλά διευκόλυνει την εισαγώγη πολλαπλών άρθρων για parse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005fc5d-70e7-4135-a1be-c0eecd02292b",
   "metadata": {},
   "source": [
    "Στη συνέχεια, χρησιμοποιούμε τις βιβλιοθήκες που έχουμε εισάγει για να κάνουμε parse τα άρθρα και να τα αποθηκέυσουμε σε μορφή .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6447df94-7191-42a6-9a9e-fddc23f16673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 has been saved to 'wikipedia_article_1.csv'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        # Fetch the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the first paragraph\n",
    "        paragraphs = soup.find_all('p')\n",
    "        parsed_paragraphs = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "\n",
    "        # Save paragraphs to a separate CSV file for each article\n",
    "        file_name = f\"wikipedia_article_{i + 1}.csv\"\n",
    "        df = pd.DataFrame({'Paragraph': parsed_paragraphs})\n",
    "        df.to_csv(file_name, index=False, encoding='utf-8')\n",
    "\n",
    "        print(f\"Article {i + 1} has been saved to '{file_name}'\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c04592-48d4-44f5-ae2c-06fa8d5c6123",
   "metadata": {},
   "source": [
    "Το παραπάνω πρόγραμμα επομένως, αποτελεί το εργαλείο με το οποίο θα αποθηκεύσουμε το σύνολο δεδομένων που επιλέξαμε, προτού προχωρήσουμε στην προεπεξεργασία του κειμένου με άλλο πρόγραμμα."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
